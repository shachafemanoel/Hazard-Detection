# Unified Dockerfile for Hazard Detection - All Services in One Container
# Multi-stage build for optimal size and performance
FROM node:20-alpine AS node-builder

WORKDIR /app

# Install Node.js dependencies
COPY package*.json ./
RUN npm install --omit=dev && npm cache clean --force

# Stage 2: Python Builder with intelligent model support
FROM python:3.11-slim AS python-builder

WORKDIR /app

# Install Python dependencies and OpenVINO
RUN apt-get update && apt-get install -y \
    gcc g++ cmake pkg-config wget curl \
    libhdf5-dev libgl1-mesa-glx libglib2.0-0 \
    libsm6 libxext6 libxrender-dev libgomp1 \
    libusb-1.0-0-dev libgtk-3-dev libavcodec-dev \
    libavformat-dev libswscale-dev libv4l-dev \
    && rm -rf /var/lib/apt/lists/*

# Create a non-root user
RUN useradd -m -s /bin/bash appuser && \
    mkdir -p /home/appuser/.local/bin && \
    chown -R appuser:appuser /home/appuser

# Add local bin to PATH
ENV PATH=/home/appuser/.local/bin:$PATH

# Create comprehensive requirements combining all needs
RUN cat > /app/unified-requirements.txt << EOF
# Core FastAPI and web server dependencies
fastapi==0.104.1
uvicorn[standard]==0.24.0
python-multipart==0.0.6
pydantic==2.5.0

# AI/ML Dependencies - OpenVINO (primary)
openvino==2025.1.0
numpy==1.24.3
opencv-python-headless==4.8.1.78
pillow==10.0.1

# AI/ML Dependencies - PyTorch (fallback)
torch>=2.0.0
torchvision>=0.15.0
ultralytics>=8.0.0

# Additional ML utilities
py-cpuinfo>=9.0.0
psutil>=5.9.0

# API and connectivity
aiohttp==3.9.1
redis==5.0.1

# Image processing and utilities
scikit-image>=0.20.0
matplotlib>=3.7.0
EOF

# Switch to non-root user before pip install
USER appuser
RUN pip install --no-cache-dir --user -r /app/unified-requirements.txt
USER root

# Stage 3: Final Runtime with intelligent CPU detection
FROM python:3.11-slim

# Install runtime dependencies and tools
RUN apt-get update && apt-get install -y \
    curl bash procps htop \
    libgl1-mesa-glx libglib2.0-0 libsm6 \
    libxext6 libxrender-dev libgomp1 \
    libusb-1.0-0 cpuid util-linux \
    && rm -rf /var/lib/apt/lists/*

# Install Node.js in the final stage
RUN curl -fsSL https://deb.nodesource.com/setup_20.x | bash - \
    && apt-get install -y nodejs \
    && rm -rf /var/lib/apt/lists/*

WORKDIR /app

# Create non-root user in final stage
RUN useradd -m -s /bin/bash appuser \
    && mkdir -p /home/appuser/.local/bin \
    && chown -R appuser:appuser /home/appuser

    # Copy Python dependencies from builder to non-root user
    COPY --from=python-builder /app/unified-requirements.txt /app/unified-requirements.txt
    RUN pip install --no-cache-dir --user -r /app/unified-requirements.txt
    ENV PATH=/home/appuser/.local/bin:$PATH
    # Copy Node.js dependencies from builder
COPY --from=node-builder /app/node_modules ./node_modules
COPY --from=node-builder /app/package*.json ./

# Install Node.js process management tools
RUN npm install -g pm2 concurrently

# Create proper directory structure
RUN mkdir -p /app/public /app/server /app/api /app/scripts

# Copy application files with their original structure
COPY api/ ./api/
COPY server/ ./server/
COPY public/ ./public/

    # Create symbolic links for unified model access
    RUN mkdir -p /app/models/openvino /app/models/pytorch \
        && rm -rf /app/models/openvino/* /app/models/pytorch/* \
        && ln -s /app/api/best_openvino_model/* /app/models/openvino/ \
        && ln -s /app/public/object_detecion_model/* /app/models/pytorch/ \
        && ln -s /app/api/best.pt /app/models/pytorch/best.pt \
        && chown -R appuser:appuser /app/models# Create CPU detection and model selection script
COPY <<EOF /app/scripts/detect-cpu-and-select-model.py
#!/usr/bin/env python3
"""
Intelligent CPU detection and model selection for Hazard Detection
Detects CPU capabilities and selects the best available model format
"""

import os
import sys
import json
import subprocess
import logging
from pathlib import Path

# Set up logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class CPUModelSelector:
    def __init__(self):
        self.app_root = Path('/app')
        self.models_root = self.app_root / 'models'
        self.config_file = self.app_root / 'model-config.json'
        
    def detect_cpu_capabilities(self):
        """Detect CPU capabilities for OpenVINO support"""
        try:
            # Try to import cpuinfo for detailed CPU detection
            try:
                from cpuinfo import get_cpu_info
                cpu_info = get_cpu_info()
                cpu_flags = cpu_info.get('flags', [])
                cpu_brand = cpu_info.get('brand_raw', 'Unknown')
                logger.info(f"CPU: {cpu_brand}")
                logger.info(f"CPU Flags: {', '.join(cpu_flags[:10])}...")  # First 10 flags
                
                # Check for OpenVINO-friendly instruction sets
                openvino_friendly = any(flag in cpu_flags for flag in [
                    'sse4_2', 'sse4.2', 'avx', 'avx2', 'avx512f'
                ])
                
                return {
                    'cpu_brand': cpu_brand,
                    'has_sse42': 'sse4_2' in cpu_flags or 'sse4.2' in cpu_flags,
                    'has_avx': 'avx' in cpu_flags,
                    'has_avx2': 'avx2' in cpu_flags,
                    'has_avx512': 'avx512f' in cpu_flags,
                    'openvino_compatible': openvino_friendly,
                    'total_flags': len(cpu_flags)
                }
                
            except ImportError:
                logger.warning("py-cpuinfo not available, using fallback detection")
                # Fallback: try to use system commands
                try:
                    result = subprocess.run(['lscpu'], capture_output=True, text=True, timeout=5)
                    lscpu_output = result.stdout.lower()
                    
                    # Basic detection from lscpu
                    has_sse42 = 'sse4_2' in lscpu_output or 'sse4.2' in lscpu_output
                    has_avx = 'avx' in lscpu_output
                    has_avx2 = 'avx2' in lscpu_output
                    
                    return {
                        'cpu_brand': 'Unknown (detected via lscpu)',
                        'has_sse42': has_sse42,
                        'has_avx': has_avx,
                        'has_avx2': has_avx2,
                        'has_avx512': False,
                        'openvino_compatible': has_sse42 or has_avx,
                        'total_flags': 'unknown'
                    }
                except Exception as e:
                    logger.warning(f"lscpu fallback failed: {e}")
                    
        except Exception as e:
            logger.error(f"CPU detection failed: {e}")
            
        # Ultimate fallback - assume basic compatibility
        return {
            'cpu_brand': 'Unknown',
            'has_sse42': True,  # Most modern CPUs have this
            'has_avx': False,
            'has_avx2': False,
            'has_avx512': False,
            'openvino_compatible': True,  # Optimistic assumption
            'total_flags': 'unknown'
        }
    
    def check_model_files(self):
        """Check availability of different model formats"""
        models = {
            'openvino': {
                'available': False,
                'path': None,
                'files': []
            },
            'pytorch': {
                'available': False,
                'path': None,
                'files': []
            },
            'onnx': {
                'available': False,
                'path': None,
                'files': []
            }
        }
        
        # Check OpenVINO models
        openvino_path = self.models_root / 'openvino'
        if openvino_path.exists():
            xml_files = list(openvino_path.glob('*.xml'))
            bin_files = list(openvino_path.glob('*.bin'))
            if xml_files and bin_files:
                models['openvino']['available'] = True
                models['openvino']['path'] = str(openvino_path)
                models['openvino']['files'] = [f.name for f in xml_files + bin_files]
        
        # Check PyTorch models
        pytorch_path = self.models_root / 'pytorch'
        if pytorch_path.exists():
            pt_files = list(pytorch_path.glob('*.pt'))
            if pt_files:
                models['pytorch']['available'] = True
                models['pytorch']['path'] = str(pytorch_path)
                models['pytorch']['files'] = [f.name for f in pt_files]
        
        # Check ONNX models (backup frontend models)
        onnx_files = list(pytorch_path.glob('*.onnx')) if pytorch_path.exists() else []
        if onnx_files:
            models['onnx']['available'] = True
            models['onnx']['path'] = str(pytorch_path)
            models['onnx']['files'] = [f.name for f in onnx_files]
        
        return models
    
    def select_optimal_configuration(self):
        """Select the best model and configuration based on CPU and available models"""
        cpu_caps = self.detect_cpu_capabilities()
        available_models = self.check_model_files()
        
        logger.info("=== CPU Detection Results ===")
        logger.info(f"CPU: {cpu_caps['cpu_brand']}")
        logger.info(f"OpenVINO Compatible: {cpu_caps['openvino_compatible']}")
        logger.info(f"SSE4.2: {cpu_caps['has_sse42']}, AVX: {cpu_caps['has_avx']}, AVX2: {cpu_caps['has_avx2']}")
        
        logger.info("=== Available Models ===")
        for model_type, info in available_models.items():
            status = "âœ… Available" if info['available'] else "âŒ Not Available"
            logger.info(f"{model_type.upper()}: {status}")
            if info['available']:
                logger.info(f"  Path: {info['path']}")
                logger.info(f"  Files: {', '.join(info['files'])}")
        
        # Decision logic for optimal configuration
        config = {
            'cpu_capabilities': cpu_caps,
            'available_models': available_models,
            'selected_backend': 'pytorch',  # Default fallback
            'model_path': None,
            'reasons': []
        }
        
        # Priority 1: OpenVINO if CPU supports it and models are available
        if (cpu_caps['openvino_compatible'] and 
            available_models['openvino']['available']):
            config['selected_backend'] = 'openvino'
            config['model_path'] = available_models['openvino']['path']
            config['reasons'].append('CPU supports OpenVINO instruction sets')
            config['reasons'].append('OpenVINO models are available')
            
        # Priority 2: PyTorch if available (universal compatibility)
        elif available_models['pytorch']['available']:
            config['selected_backend'] = 'pytorch'
            config['model_path'] = available_models['pytorch']['path']
            config['reasons'].append('PyTorch models available (universal compatibility)')
            if not cpu_caps['openvino_compatible']:
                config['reasons'].append('CPU lacks OpenVINO-optimal instruction sets')
                
        # Priority 3: ONNX as last resort for frontend
        elif available_models['onnx']['available']:
            config['selected_backend'] = 'onnx'
            config['model_path'] = available_models['onnx']['path']
            config['reasons'].append('Using ONNX models as fallback')
            config['reasons'].append('Neither OpenVINO nor PyTorch models available')
        
        else:
            config['reasons'].append('No suitable models found!')
            logger.error("âŒ No suitable models found for any backend!")
        
        return config
    
    def generate_environment_config(self, config):
        """Generate environment variables for the selected configuration"""
        env_vars = {
            'MODEL_BACKEND': config['selected_backend'],
            'MODEL_DIR': config['model_path'] or '/app/models',
            'CPU_OPTIMIZATION': 'enabled' if config['cpu_capabilities']['openvino_compatible'] else 'basic',
            'INFERENCE_MODE': config['selected_backend'],
        }
        
        # Backend-specific configurations
        if config['selected_backend'] == 'openvino':
            env_vars.update({
                'OPENVINO_MODEL_XML': f"{config['model_path']}/best.xml",
                'OPENVINO_MODEL_BIN': f"{config['model_path']}/best.bin",
                'OV_CPU_THREADS': '0',  # Auto-detect
                'OV_CACHE_ENABLED': 'true'
            })
        elif config['selected_backend'] == 'pytorch':
            pytorch_model = None
            if config['model_path']:
                pt_files = list(Path(config['model_path']).glob('*.pt'))
                pytorch_model = str(pt_files[0]) if pt_files else None
            
            env_vars.update({
                'PYTORCH_MODEL_PATH': pytorch_model or '/app/models/pytorch/best.pt',
                'TORCH_NUM_THREADS': '0',  # Auto-detect
                'TORCH_DEVICE': 'cpu'
            })
        
        return env_vars
    
    def save_config(self):
        """Main method to detect, select, and save configuration"""
        try:
            logger.info("ðŸ” Starting intelligent model selection...")
            
            config = self.select_optimal_configuration()
            env_vars = self.generate_environment_config(config)
            
            # Save detailed config
            full_config = {
                'timestamp': str(subprocess.check_output(['date'], text=True).strip()),
                'selection_config': config,
                'environment_variables': env_vars,
                'container_info': {
                    'python_version': sys.version,
                    'working_directory': str(self.app_root)
                }
            }
            
            with open(self.config_file, 'w') as f:
                json.dump(full_config, f, indent=2)
            
            # Save environment file for Docker
            env_file = self.app_root / '.env.model'
            with open(env_file, 'w') as f:
                f.write("# Auto-generated model configuration\\n")
                f.write(f"# Generated at: {full_config['timestamp']}\\n")
                f.write(f"# Selected backend: {config['selected_backend']}\\n")
                for key, value in env_vars.items():
                    f.write(f"{key}={value}\\n")
            
            logger.info("âœ… Model selection completed successfully!")
            logger.info(f"ðŸ“Š Selected Backend: {config['selected_backend'].upper()}")
            logger.info(f"ðŸ“ Model Path: {config['model_path']}")
            logger.info("ðŸ“ Reasons:")
            for reason in config['reasons']:
                logger.info(f"   â€¢ {reason}")
            
            return config
            
        except Exception as e:
            logger.error(f"âŒ Model selection failed: {e}")
            # Create fallback configuration
            fallback_env = {
                'MODEL_BACKEND': 'pytorch',
                'MODEL_DIR': '/app/models/pytorch',
                'PYTORCH_MODEL_PATH': '/app/models/pytorch/best.pt',
                'INFERENCE_MODE': 'pytorch'
            }
            
            with open(self.app_root / '.env.model', 'w') as f:
                f.write("# Fallback configuration\\n")
                for key, value in fallback_env.items():
                    f.write(f"{key}={value}\\n")
            
            return {'selected_backend': 'pytorch', 'model_path': '/app/models/pytorch'}

if __name__ == '__main__':
    selector = CPUModelSelector()
    result = selector.save_config()
    print(f"Selected backend: {result['selected_backend']}")
    sys.exit(0)
EOF

# Create the unified startup script with intelligent model loading
COPY <<EOF /app/start-unified.sh
#!/bin/bash
set -e

echo "ðŸš€ Starting Unified Hazard Detection Service..."
echo "ðŸ³ Container Environment: \${NODE_ENV:-production}"
echo "ðŸŒ Primary Port: \${PORT:-8080}"

# Function to cleanup on exit
cleanup() {
    echo "ðŸ›‘ Shutting down all services..."
    pkill -f "uvicorn" 2>/dev/null || true
    pkill -f "node" 2>/dev/null || true
    pkill -f "pm2" 2>/dev/null || true
    exit 0
}

# Set up signal handlers
trap cleanup SIGTERM SIGINT EXIT

# Step 1: Run intelligent model selection
echo "ðŸ§  Running intelligent CPU detection and model selection..."
cd /app
python3 /app/scripts/detect-cpu-and-select-model.py

# Load the generated model configuration
if [ -f "/app/.env.model" ]; then
    echo "ðŸ“‹ Loading model configuration..."
    export \$(cat /app/.env.model | grep -v '^#' | xargs)
    echo "âœ… Selected Backend: \${MODEL_BACKEND}"
    echo "ðŸ“ Model Directory: \${MODEL_DIR}"
else
    echo "âš ï¸ No model configuration found, using defaults..."
    export MODEL_BACKEND=pytorch
    export MODEL_DIR=/app/models/pytorch
    export PYTORCH_MODEL_PATH=/app/models/pytorch/best.pt
fi

# Step 2: Update API app configuration based on selection
echo "âš™ï¸ Configuring API service for \${MODEL_BACKEND} backend..."

# Set common environment variables
export PYTHONPATH=/app
export API_URL=http://localhost:8000
export WEB_PORT=\${PORT:-8080}

# Step 3: Start FastAPI service on port 8000
echo "ðŸ Starting FastAPI (\${MODEL_BACKEND} backend) on port 8000..."
cd /app
export MODEL_DIR=/app/models/openvino
export API_PORT=8000
uvicorn api.app:app --host 0.0.0.0 --port ${API_PORT} --workers 1 &
API_PID=\$!

# Wait for API to be ready
echo "â³ Waiting for API service to start..."
for i in {1..30}; do
    if curl -f http://localhost:8000/health >/dev/null 2>&1; then
        echo "âœ… API service is ready!"
        break
    fi
    if [ \$i -eq 30 ]; then
        echo "âŒ API service failed to start within 30 seconds"
        exit 1
    fi
    sleep 1
done

# Step 4: Start Express web server
echo "ðŸŒ Starting Express web server on port \${WEB_PORT}..."
cd /app/server/routes

if [ -f "server.js" ]; then
    # Use the full-featured server with authentication
    API_URL=http://localhost:8000 PORT=\${WEB_PORT} node server.js &
    WEB_PID=\$!
elif [ -f "simple-server.js" ]; then
    # Fallback to simple server
    echo "âš ï¸ Using simple server (full server.js not found)"
    API_URL=http://localhost:8000 PORT=\${WEB_PORT} node simple-server.js &
    WEB_PID=\$!
else
    echo "âŒ No web server found in \$(pwd)"
    echo "ðŸ“ Available files:"
    ls -la
    exit 1
fi

# Step 5: Health check and monitoring
echo "ðŸ¥ Starting health monitoring..."
for i in {1..20}; do
    if curl -f http://localhost:\${WEB_PORT}/health >/dev/null 2>&1; then
        echo "âœ… Web server is ready!"
        break
    fi
    if [ \$i -eq 20 ]; then
        echo "âŒ Web server failed to start within 20 seconds"
        exit 1
    fi
    sleep 1
done

echo "ðŸŽ‰ All services started successfully!"
echo "ðŸ“Š Service Status:"
echo "   ðŸ FastAPI (AI Backend): http://localhost:8000 (PID: \$API_PID)"
echo "   ðŸŒ Web Server: http://localhost:\${WEB_PORT} (PID: \$WEB_PID)"
echo "   ðŸ§  AI Backend: \${MODEL_BACKEND}"
echo "   ðŸ“ Model Path: \${MODEL_DIR}"

# Create a simple health check endpoint info
cat > /tmp/service-info.json << EOL
{
  "status": "running",
  "services": {
    "api": {
      "url": "http://localhost:8000",
      "pid": \$API_PID,
      "backend": "\${MODEL_BACKEND}",
      "model_path": "\${MODEL_DIR}"
    },
    "web": {
      "url": "http://localhost:\${WEB_PORT}",
      "pid": \$WEB_PID
    }
  },
  "started_at": "\$(date -Iseconds)"
}
EOL

echo "ðŸ”„ Services are running. Press Ctrl+C to stop."

# Wait for both processes
wait
EOF

RUN chmod +x /app/scripts/detect-cpu-and-select-model.py
RUN chmod +x /app/start-unified.sh

# Set default environment variables
ENV NODE_ENV=production
ENV PYTHONPATH=/app
ENV PORT=8080
ENV API_URL=http://localhost:8000
ENV API_PORT=8000

# Create health check script
COPY <<EOF /app/health-check.sh
#!/bin/bash
# Check both services are running
if curl -f http://localhost:8000/health >/dev/null 2>&1 && \\
   curl -f http://localhost:\${PORT:-8080}/health >/dev/null 2>&1; then
    exit 0
else
    exit 1
fi
EOF

RUN chmod +x /app/health-check.sh

# Expose ports
EXPOSE 8000 8080

# Health check for the unified container
HEALTHCHECK --interval=30s --timeout=15s --start-period=60s --retries=3 \
    CMD /app/health-check.sh

# Switch to non-root user for running the application
USER appuser

# Use the intelligent startup script
CMD ["/app/start-unified.sh"]